#!/usr/bin/env python3
"""
Fixed Ultra-Precision Test with Proper Class Mapping
"""
import torch
from ultralytics import YOLO
import cv2
import numpy as np
from pathlib import Path
import json
import time
from collections import defaultdict

def test_fixed_precision():
    """Test with corrected class mapping and stricter filtering"""
    print("üîß Testing FIXED Ultra-Precision Setup...")
    
    # Load only our trained model for accurate results
    device = 0 if torch.cuda.is_available() else 'cpu'
    print(f"üöÄ Using {'GPU (RTX 3050)' if device == 0 else 'CPU'}")
    
    model_path = 'runs/high_precision/weights/best.pt'
    if not Path(model_path).exists():
        print("‚ùå High-precision model not found! Training needed.")
        return
    
    try:
        model = YOLO(model_path)
        model.to(device)
        print("‚úÖ Loaded YOLOv8m High-Precision Model")
    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        return
    
    # Correct class names for our dataset
    class_names = [
        'OxygenTank',
        'NitrogenTank', 
        'FirstAidBox',
        'FireAlarm',
        'SafetySwitchPanel',
        'EmergencyPhone',
        'FireExtinguisher'
    ]
    
    # Test on sample images
    test_images = list(Path('data/test/images').glob('*.png'))[:10]
    print(f"\nüß™ Testing on {len(test_images)} images...\n")
    
    total_detections = 0
    total_confidence = 0.0
    valid_detections = 0
    processing_times = []
    
    for img_path in test_images:
        print(f"üì∏ Testing: {img_path.name}")
        
        # Load image
        img = cv2.imread(str(img_path))
        if img is None:
            continue
        
        start_time = time.time()
        
        # Single model with multiple preprocessing
        all_detections = []
        
        # Enhanced preprocessing for better detection
        variants = [
            ('original', img),
            ('enhanced_contrast', enhance_contrast(img)),
            ('brightened', cv2.convertScaleAbs(img, alpha=1.2, beta=20)),
            ('darkened', cv2.convertScaleAbs(img, alpha=0.8, beta=-10))
        ]
        
        for variant_name, variant_img in variants:
            try:
                # High-precision inference settings
                results = model(
                    variant_img,
                    conf=0.25,  # Higher confidence threshold
                    iou=0.45,   # Standard IoU
                    max_det=20,  # Reasonable max detections
                    augment=True,  # Test-time augmentation
                    verbose=False
                )
                
                if results and len(results) > 0 and results[0].boxes is not None:
                    detections = extract_valid_detections(results[0], class_names, variant_name)
                    all_detections.extend(detections)
                    
            except Exception as e:
                print(f"   ‚ö†Ô∏è Error with {variant_name}: {e}")
                continue
        
        # Apply smart NMS to remove duplicates
        final_detections = smart_nms(all_detections)
        
        processing_time = time.time() - start_time
        processing_times.append(processing_time)
        
        # Calculate metrics
        valid_dets = [d for d in final_detections if d['class_id'] < 7]  # Only our 7 classes
        
        if valid_dets:
            avg_conf = sum(det['confidence'] for det in valid_dets) / len(valid_dets)
            precision_score = calculate_better_precision(valid_dets, len(all_detections))
        else:
            avg_conf = 0.0
            precision_score = 0.0
        
        total_detections += len(valid_dets)
        valid_detections += len(valid_dets)
        total_confidence += avg_conf
        
        # Display results
        print(f"   üéØ Valid Detections: {len(valid_dets)} (Total: {len(final_detections)})")
        print(f"   üìä Avg Confidence: {avg_conf:.1%}")
        print(f"   üé™ Precision Score: {precision_score:.3f} ({get_grade(precision_score)})")
        print(f"   ‚è±Ô∏è  Processing: {processing_time:.2f}s")
        
        # Show top detections
        top_detections = sorted(valid_dets, key=lambda x: x['confidence'], reverse=True)[:3]
        for det in top_detections:
            print(f"      {det['class_name']}: {det['confidence']:.1%} ({det['variant']})")
        print()
    
    # Overall statistics
    if test_images:
        avg_detections = valid_detections / len(test_images)
        avg_confidence = total_confidence / len(test_images) if total_confidence > 0 else 0
        avg_time = sum(processing_times) / len(processing_times)
        
        print("üìä CORRECTED PERFORMANCE:")
        print(f"   Average Valid Detections per Image: {avg_detections:.1f}")
        print(f"   Average Confidence: {avg_confidence:.1%}")
        print(f"   Average Processing Time: {avg_time:.2f}s")
        print(f"   GPU Acceleration: {'Yes (RTX 3050)' if device == 0 else 'No'}")
        
        # More accurate improvement calculation
        if avg_confidence > 0:
            estimated_map50 = min(0.95, 0.60 + (avg_confidence - 0.25) * 0.8)  # More conservative
            grade = get_grade(estimated_map50)
            
            print(f"\nüöÄ ESTIMATED PERFORMANCE:")
            print(f"   Estimated mAP50: {estimated_map50:.1%}")
            print(f"   Performance Grade: {grade}")
            
            if estimated_map50 > 0.744:  # Better than baseline
                improvement = ((estimated_map50 - 0.744) / 0.744) * 100
                print(f"   Improvement over baseline: +{improvement:.1f}%")
            else:
                decline = ((0.744 - estimated_map50) / 0.744) * 100
                print(f"   Performance vs baseline: -{decline:.1f}%")
                print("   üìù Note: Lower confidence threshold may help")
        else:
            print("   ‚ö†Ô∏è No valid detections - model may need retraining")

def enhance_contrast(img):
    """Enhanced contrast with CLAHE"""
    lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))
    l = clahe.apply(l)
    enhanced = cv2.merge([l, a, b])
    return cv2.cvtColor(enhanced, cv2.COLOR_LAB2BGR)

def extract_valid_detections(results, class_names, variant_name):
    """Extract only valid class detections"""
    detections = []
    
    if results.boxes is not None:
        boxes = results.boxes.xyxy.cpu().numpy()
        confidences = results.boxes.conf.cpu().numpy()
        class_ids = results.boxes.cls.cpu().numpy().astype(int)
        
        for box, conf, cls_id in zip(boxes, confidences, class_ids):
            # Only keep detections for our 7 safety equipment classes
            if 0 <= cls_id < len(class_names):
                detection = {
                    'bbox': box.tolist(),
                    'confidence': float(conf),
                    'class_id': int(cls_id),
                    'class_name': class_names[cls_id],
                    'variant': variant_name
                }
                detections.append(detection)
    
    return detections

def smart_nms(detections, iou_threshold=0.5):
    """Smart NMS that considers both IoU and confidence"""
    if not detections:
        return []
    
    # Group by class
    class_groups = defaultdict(list)
    for det in detections:
        class_groups[det['class_id']].append(det)
    
    final_detections = []
    
    for class_id, class_dets in class_groups.items():
        # Sort by confidence
        class_dets.sort(key=lambda x: x['confidence'], reverse=True)
        
        keep = []
        while class_dets:
            best = class_dets.pop(0)
            
            # Check if this detection is too similar to existing ones
            is_duplicate = False
            for kept in keep:
                iou = calculate_iou(best['bbox'], kept['bbox'])
                if iou > iou_threshold:
                    # Merge if very similar and high confidence
                    if iou > 0.7 and abs(best['confidence'] - kept['confidence']) < 0.2:
                        # Update kept detection with higher confidence
                        if best['confidence'] > kept['confidence']:
                            kept.update(best)
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                keep.append(best)
            
            # Remove similar detections from remaining
            remaining = []
            for det in class_dets:
                if calculate_iou(best['bbox'], det['bbox']) < iou_threshold:
                    remaining.append(det)
            class_dets = remaining
        
        final_detections.extend(keep)
    
    return final_detections

def calculate_iou(box1, box2):
    """Calculate IoU between two boxes"""
    x1_inter = max(box1[0], box2[0])
    y1_inter = max(box1[1], box2[1])
    x2_inter = min(box1[2], box2[2])
    y2_inter = min(box1[3], box2[3])
    
    if x2_inter <= x1_inter or y2_inter <= y1_inter:
        return 0.0
    
    intersection = (x2_inter - x1_inter) * (y2_inter - y1_inter)
    area1 = (box1[2] - box1[0]) * (box1[3] - box1[1])
    area2 = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union = area1 + area2 - intersection
    
    return intersection / union if union > 0 else 0.0

def calculate_better_precision(detections, raw_count):
    """Better precision calculation"""
    if not detections:
        return 0.0
    
    # Base score from confidence
    avg_confidence = sum(det['confidence'] for det in detections) / len(detections)
    
    # Bonus for consistent detections (same object detected in multiple variants)
    variant_groups = defaultdict(list)
    for det in detections:
        key = f"{det['class_id']}_{int(det['bbox'][0]//50)}_{int(det['bbox'][1]//50)}"
        variant_groups[key].append(det)
    
    consistency_bonus = 0
    for group in variant_groups.values():
        if len(group) > 1:  # Same object detected in multiple variants
            consistency_bonus += 0.05
    
    precision_score = min(1.0, avg_confidence + consistency_bonus)
    return precision_score

def get_grade(score):
    """Convert score to letter grade"""
    if score >= 0.95: return "A+"
    elif score >= 0.90: return "A"
    elif score >= 0.85: return "A-"
    elif score >= 0.80: return "B+"
    elif score >= 0.75: return "B"
    elif score >= 0.70: return "B-"
    else: return "C"

if __name__ == "__main__":
    test_fixed_precision()